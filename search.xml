<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>解密强化学习的“北极星”：深入理解RL的目标函数</title>
      <link href="/2025/09/29/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3RL%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/"/>
      <url>/2025/09/29/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3RL%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>在探索强化学习（Reinforcement Learning, RL）的广袤世界时，我们常常会遇到各种复杂的算法和概念。但所有这些算法，无论多么不同，都在追寻同一个目标，遵循同一个“北极星”的指引。这个“北极星”就是RL的<strong>目标函数</strong>。</p><p>今天，我们将深入剖析这个在RL理论中至关重要的核心公式，理解智能体（Agent）学习的真正意义。</p><p>首先，让我们请出今天的主角——RL的优化目标函数：</p>$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=1}^{\infty} \gamma^{t-1} r(s_t, a_t) \right] = V_{\pi_\theta}(s_1) = \sum_{a \in A} \pi_\theta(a|s_1) Q_{\pi_\theta}(s_1, a)$$<p>这个公式看起来可能有些令人生畏，但别担心，我们将把它拆解成三个逻辑清晰的部分，一步步揭开它的神秘面纱。</p><h2 id="第一部分：定义终极目标——最大化期望回报"><a href="#第一部分：定义终极目标——最大化期望回报" class="headerlink" title="第一部分：定义终极目标——最大化期望回报"></a>第一部分：定义终极目标——最大化期望回报</h2><p>公式的第一部分定义了我们最终要优化的目标是什么：</p>$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=1}^{\infty} \gamma^{t-1} r(s_t, a_t) \right]$$<p>这可以理解为对智能体策略“性能”的数学描述。让我们逐一解析其中的符号：</p><ul><li>$J(\theta)$: <strong>目标函数</strong>，也就是我们衡量策略好坏的“分数”。$\theta$ 是策略 $\pi_\theta$ 的参数（例如神经网络的权重），我们的目标就是通过调整 $\theta$ 来让 $J(\theta)$ 最大化。</li><li>$\pi_\theta$: 智能体的<strong>策略 (Policy)</strong>，即智能体的“行动指南”或“大脑”。它根据当前状态 $s$ 决定采取哪个动作 $a$。</li><li>$\tau \sim \pi<em>\theta$: $\tau$ 代表一条<strong>轨迹 (Trajectory)</strong>，也就是智能体与环境从开始到结束交互的全过程记录，形如 $\tau = (s_1, a_1, r_1, s_2, a_2, r_2, \dots)$。下标 $\tau \sim \pi</em>\theta$ 意味着这条轨迹是遵循策略 $\pi_\theta$ 产生的。</li><li>$\mathbb{E}[\cdot]$: <strong>期望 (Expectation)</strong>。由于策略和环境都可能存在随机性，每次“游戏”的轨迹都可能不同。为了客观评价一个策略的好坏，我们不能只看一次表现，而是要计算它在所有可能轨迹下的<strong>平均表现</strong>。</li><li>$\sum_{t=1}^{\infty} \gamma^{t-1} r(s_t, a_t)$: 这是整条轨迹的<strong>折扣累积奖励 (Discounted Cumulative Reward)</strong>，也称为<strong>回报 (Return)</strong>。<ul><li>$r(s_t, a_t)$ 是在 $t$ 时刻获得的<strong>即时奖励</strong>。</li><li>$\gamma$ 是<strong>折扣因子</strong> ($0 &lt; \gamma \le 1$)，它决定了我们对未来奖励的重视程度。$\gamma$ 越小，智能体越“短视”，只关心眼前利益；$\gamma$ 越大，智能体越有“远见”，会为了长远目标牺牲短期利益。</li></ul></li></ul><p><strong>一句话总结</strong>：RL的目标是找到一个最优策略 $\pi_{\theta^<em>}$，使得在该策略下与环境交互时，能够获得的<em>*平均总回报</em></em>最大化。</p><h2 id="第二部分：从“目标”到“价值”——与V函数的关联"><a href="#第二部分：从“目标”到“价值”——与V函数的关联" class="headerlink" title="第二部分：从“目标”到“价值”——与V函数的关联"></a>第二部分：从“目标”到“价值”——与V函数的关联</h2><p>公式的第二部分将这个宏观的目标与RL中一个我们更熟悉的概念联系起来：</p>$$J(\theta) = V_{\pi_\theta}(s_1)$$<ul><li>$V<em>{\pi</em>\theta}(s)$: <strong>状态价值函数 (State-Value Function)</strong>。它评估的是，从某个状态 $s$ 出发，如果接下来一直遵循策略 $\pi_\theta$，未来能获得的期望回报是多少。它衡量了一个状态的“长期潜力”或“好坏程度”。</li><li>$s_1$: 特指任务的<strong>初始状态</strong>。</li></ul><p>这个等式告诉我们一个非常直观的道理：<strong>一个策略的整体好坏，就等于它从起点出发所能创造的长期价值。</strong></p><p>这就像评价一个棋手的开局策略，我们最终看的是这个开局所导向的局面的优劣程度。最大化策略的性能 $J(\theta)$，就等同于最大化初始状态的价值 $V<em>{\pi</em>\theta}(s_1)$。</p><h2 id="第三部分：从“状态价值”到“动作价值”——与Q函数的关联"><a href="#第三部分：从“状态价值”到“动作价值”——与Q函数的关联" class="headerlink" title="第三部分：从“状态价值”到“动作价值”——与Q函数的关联"></a>第三部分：从“状态价值”到“动作价值”——与Q函数的关联</h2><p>公式的最后一部分，则进一步将状态的价值分解为具体动作的价值：</p>$$V_{\pi_\theta}(s_1) = \sum_{a \in A} \pi_\theta(a|s_1) Q_{\pi_\theta}(s_1, a)$$<ul><li>$Q<em>{\pi</em>\theta}(s, a)$: <strong>动作价值函数 (Action-Value Function)</strong>。它评估的是，在状态 $s$ 下，<strong>选择了某个具体的动作 $a$</strong> 之后，再继续遵循策略 $\pi_\theta$ 能获得的期望回报。它衡量了一个“状态-动作”对的“长期潜力”。</li><li>$\pi_\theta(a|s_1)$: 策略在初始状态 $s_1$ 时，选择动作 $a$ 的概率。</li></ul><p>这个等式揭示了 V 函数和 Q 函数的内在关系：<strong>一个状态的价值，等于从这个状态出发所有可能动作的价值的加权平均，而权重就是策略选择每个动作的概率。</strong></p><p><strong>举个例子</strong>：<br>假设在初始状态 $s_1$，你有两个选择：</p><ol><li>动作 $a<em>{\text{向前}}$ (长期价值 $Q(s_1, a</em>{\text{向前}}) = 10$)</li><li>动作 $a<em>{\text{向后}}$ (长期价值 $Q(s_1, a</em>{\text{向后}}) = -5$)</li></ol><p>如果你的策略是 70% 的概率向前，30% 的概率向后，那么当前状态 $s_1$ 的价值就是：<br>$V(s_1) = 0.7 \times 10 + 0.3 \times (-5) = 7 - 1.5 = 5.5$</p><h2 id="总结：一条贯穿RL的逻辑链"><a href="#总结：一条贯穿RL的逻辑链" class="headerlink" title="总结：一条贯穿RL的逻辑链"></a>总结：一条贯穿RL的逻辑链</h2><p>现在，让我们把这三部分串起来，回顾这条清晰的逻辑链：</p><ol><li><strong>终极目标</strong>：我们的使命是调整策略参数 $\theta$ 来最大化目标函数 $J(\theta)$。</li><li><strong>目标是什么</strong>：$J(\theta)$ 被定义为遵循该策略时，智能体能获得的<strong>平均总回报</strong>。</li><li><strong>目标如何衡量</strong>：这个平均总回报，等价于<strong>初始状态的价值 $V<em>{\pi</em>\theta}(s_1)$</strong>。</li><li><strong>价值如何计算</strong>：一个状态的价值，又可以被分解为该状态下所有<strong>动作价值 $Q<em>{\pi</em>\theta}(s_1, a)$ 的期望</strong>。</li></ol><p>这条公式完美地将RL的宏观目标（最大化长期回报）与算法在微观层面需要计算和优化的量（V值和Q值）联系在了一起。它不仅是理论的基石，更是许多现代强化学习算法（如策略梯度、Actor-Critic等）设计的出发点。</p><p>理解了它，你就掌握了解读大部分RL算法设计思想的钥匙。它就是指引我们在强化学习道路上不断前进的“北极星”。</p>]]></content>
      
      
      <categories>
          
          <category> 技术分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>鼠鼠的保研奇幻之旅</title>
      <link href="/2025/09/27/%E9%BC%A0%E9%BC%A0%E7%9A%84%E4%BF%9D%E7%A0%94%E5%A5%87%E5%B9%BB%E4%B9%8B%E6%97%85/"/>
      <url>/2025/09/27/%E9%BC%A0%E9%BC%A0%E7%9A%84%E4%BF%9D%E7%A0%94%E5%A5%87%E5%B9%BB%E4%B9%8B%E6%97%85/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
